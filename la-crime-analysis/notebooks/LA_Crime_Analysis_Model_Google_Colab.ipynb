# Install required libraries
!pip install --upgrade google-cloud-bigquery
!pip install --upgrade pandas
!pip install --upgrade matplotlib
!pip install --upgrade scikit-learn


from google.colab import files
import os

# Upload your Google Cloud service account key file
uploaded = files.upload()

# Get the filename of the uploaded file
key_file = next(iter(uploaded))  # This grabs the first file uploaded

# Set the environment variable for Google Cloud authentication
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = key_file  # This uses the uploaded file

from google.cloud import bigquery
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Initialize a BigQuery client
client = bigquery.Client()

# Your BigQuery query
query = """
SELECT * FROM `la-crime-analysis.la_crime_dataset.crime_data`
"""

# Run the query and convert the results into a pandas dataframe
df = client.query(query).to_dataframe()

# Drop unnecessary columns
df = df.drop(columns=['dr_no', 'date_occ', 'date_rptd'])
df = df.drop_duplicates()

# Frequency encoding for high-cardinality columns
high_cardinality_columns = ['mocodes', 'location', 'cross_street']
for col in high_cardinality_columns:
    if col in df.columns:
        frequency_encoding = df[col].value_counts(normalize=True)
        df[f'{col}_encoded'] = df[col].map(frequency_encoding).fillna(0)
        # Drop original columns to avoid confusion
        df = df.drop(columns=[col])

# Label encoding for low-cardinality columns
low_cardinality_columns = ['vict_sex', 'premis_desc', 'weapon_desc', 'status']
label_encoder = LabelEncoder()
for col in low_cardinality_columns:
    if col in df.columns:
        df[f'{col}_encoded'] = label_encoder.fit_transform(df[col])
        # Drop original columns to avoid confusion
        df = df.drop(columns=[col])

# One-hot encoding for categorical columns
df = pd.get_dummies(df, columns=['area_name', 'vict_descent', 'status_desc'], drop_first=True)

# Define features (X) and target (y)
X = df.drop(columns=['crm_cd_desc'])
y = df['crm_cd_desc']

# Encode the target variable (crm_cd_desc)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Handling missing values in other columns
# Example: Fill NaN in specific columns as needed (like 'vict_age')
X['vict_age'] = X['vict_age'].fillna(X['vict_age'].median())  # Or use the mean/advanced imputation

# Handling missing columns crm_cd_* if present
crm_cd_columns = ['crm_cd_1', 'crm_cd_2', 'crm_cd_3', 'crm_cd_4']
for col in crm_cd_columns:
    if col in X.columns:
        X[col] = X[col].fillna(0)  # Fill missing values with 0 or another appropriate value

# Add new feature for missing values in 'vict_age'
X['age_missing'] = X['vict_age'].isnull().astype(int)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Debugging: View only the columns with dtype 'object' after encoding
object_columns = X.select_dtypes(include=['object']).columns
print("Columns with dtype 'object':")
print(object_columns)

print("Data preprocessing complete. Training and test sets are ready.")


from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Example: Logistic Regression Model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict on the test data
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
